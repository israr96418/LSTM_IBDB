{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPayuBJrCfqRUo58ExB3ZvP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/israr96418/LSTM_IBDB/blob/main/LSTM_apply_on_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Y7zPDzBhJ-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c151e7-b2ac-45f0-e766-27bc36c94cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function pad_sequences at 0x7f469f43ddc0>\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 489s 1s/step - loss: 0.4733 - accuracy: 0.7699 - val_loss: 0.3578 - val_accuracy: 0.8528\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 485s 1s/step - loss: 0.3049 - accuracy: 0.8789 - val_loss: 0.4194 - val_accuracy: 0.8080\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 484s 1s/step - loss: 0.2557 - accuracy: 0.9016 - val_loss: 0.3087 - val_accuracy: 0.8718\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 487s 1s/step - loss: 0.2344 - accuracy: 0.9081 - val_loss: 0.3225 - val_accuracy: 0.8670\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 485s 1s/step - loss: 0.1902 - accuracy: 0.9287 - val_loss: 0.3169 - val_accuracy: 0.8750\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 480s 1s/step - loss: 0.1708 - accuracy: 0.9358 - val_loss: 0.3553 - val_accuracy: 0.8684\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 482s 1s/step - loss: 0.1586 - accuracy: 0.9414 - val_loss: 0.3831 - val_accuracy: 0.8688\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 488s 1s/step - loss: 0.1282 - accuracy: 0.9536 - val_loss: 0.4882 - val_accuracy: 0.8639\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 494s 1s/step - loss: 0.1346 - accuracy: 0.9501 - val_loss: 0.4038 - val_accuracy: 0.8616\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 486s 1s/step - loss: 0.1285 - accuracy: 0.9522 - val_loss: 0.4378 - val_accuracy: 0.8627\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f468dc9e730>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.datasets import imdb\n",
        "\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "# to ensure that you are using tensorflow as backend\n",
        "\"KERAS_BACKEND=tensorflow\"\n",
        "\n",
        "print(pad_sequences)\n",
        "\n",
        "# load dataset from keras but keep only the top 5000 frequency word\n",
        "# and split and shuffle  into train and test how much?\n",
        "# it will be decide automatically by deep learning\n",
        "\n",
        "top_words = 5000\n",
        "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words = top_words)\n",
        "# as we known that SGD with batch size =1 means we putt single word of every review one by one upto finishing all the reviews\n",
        "# it become very slow and time consuming\n",
        "\n",
        "\n",
        "# for that SGD with batch_size = 5 ,-->it means putt 5 words of every review upto finising all reviews\n",
        "# but for this we do padding-->let padding=600(e.g review containt 150 words so before 600-150=450 add 450 zero before )\n",
        "\n",
        "max_review_length=600\n",
        "x_train = pad_sequences(x_train, maxlen=max_review_length)\n",
        "x_test = pad_sequences(x_test, maxlen=max_review_length)\n",
        "\n",
        "\n",
        "# Create model\n",
        "# already take note in notebook in detailsabout embedded layer see theri\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "# embedded layer take positive words and convert it into dense vector(which is fixed lenth(you can specify the lenght)=32)\n",
        "# it is only added in the first layer of the model\n",
        "# mostly used in NPL task\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length = max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer=\"adam\", metrics=['accuracy'])\n",
        "# print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "# train model\n",
        "model.fit(x_train,y_train, batch_size=64 ,epochs=10, verbose=1, validation_data=(x_test,y_test))\n",
        "\n"
      ]
    }
  ]
}